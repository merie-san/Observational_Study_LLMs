%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
a4paper, % Paper size, use either a4paper or letterpaper
10pt, % Default font size, can also use 11pt or 12pt, although this is not recommended
unnumberedsections, % Comment to enable section numbering
twoside, % Two side traditional mode where headers and footers change between odd and even pages, comment this option to make them fixed
]{LTJournalArticle}

\addbibresource{paper.bib} % BibLaTeX bibliography file

\runninghead{Usage of LLMs in Github repos} % A shortened article title to appear in the running head, leave this command empty for no running head

\footertext{} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Observational Study on usage of Large Language Models in public Github repositories} % Article title, use manual lines breaks (\\) to beautify the layout

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{%
	Davide Zhang\textsuperscript{1} and Roberto Magrini\textsuperscript{1}
	\thanks{Emails: \href{mailto:davide.zhang@edu.unifi.it}{davide.zhang@edu.unifi.it} \href{mailto:roberto.magrini@edu.unifi.it}{roberto.magrini@edu.unifi.it}}
}

% Affiliations are output in the \date{} command
\date{\footnotesize\textsuperscript{\textbf{1}}Scuola di Scienze Matematiche Fisiche e Naturali, UNIVERSITÀ DEGLI STUDI FIRENZE}

% Full-width abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent The increasing integration of Large Language Models (LLMs) in software engineering, both as development tools and as components of deployed systems, raises the need to understand how developers adopt these technologies in real-world projects. This study analyzes a large set of public GitHub repositories to identify which LLM technologies they employ and how their usage correlates with other project characteristics. We combine repository metadata with code-search based mining to detect both the SDKs and the models used, enabling an examination of technology popularity, application domains, and common SDK–model combinations. Our dataset also includes monthly snapshots of the most popular repositories from 2022 to 2025. Using statistical analysis, we investigate trends in LLM adoption and explore relationships between the presence of LLM components and repository attributes such as size and activity. The results provide an empirical characterization of the current LLM landscape in open-source software development.
		
	\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}
	
	\maketitle % Output the title section
	
	%----------------------------------------------------------------------------------------
	%	ARTICLE CONTENTS
	%----------------------------------------------------------------------------------------
	
	\section{Introduction}
	
Our objective is to thoroughly analyze the usage and prominence of LLMs in GitHub repositories. Many different languages can be used to do this analysis, but we decided to only consider the following languages:
	
	\begin{itemize}
		\item \textbf{Python}, the most popular language for LLM inference through programmatic API.
		\item \textbf{Java}, a popular high-level language for the development of web applications.
		\item \textbf{Go}, similar to Java but with a simpler syntax.
	\end{itemize}
	
	We also decided to only consider the official SDK provided by the following makers:
	
	\begin{itemize}
		\item \textbf{OpenAI} 
		\item \textbf{Anthropic}
		\item \textbf{Mistral}
		\item \textbf{Google}
		\item \textbf{xAI}
		\item \textbf{Meta}
	\end{itemize}
	
	The list of available models for each maker was obtained by visiting the official site and combining results there with the return of a query to the API of the maker when the call was free.
	
	To reach our goal, we first analyzed for each language the evolution of the percentage of repositories containing LLM-adjacent words in their topic, description, or title. From the data we collected, we performed several analyses, such as which language is the most popular for the development of LLM-integrated applications or which repository metrics are more correlated. Starting from this analysis, we conducted a deeper research on public GitHub repositories by using code search to retrieve all instances of relevant SDK import or model reference. The collected data is combined to obtain SDK-model pairs that can be analyzed and used to derive insights for our study. We then sampled a set of representative repositories from this exhaustive list to apply statistical methods and infer properties of the sampled population.\\
	Our empirical analysis leads to several key findings:
	\begin{itemize}
		\item \textbf{Rapid growth of LLM-related projects}, since the introduction of LLMs to public usage in 2022 the proportion of LLM-related repositories among the most-starred projects increased significantly across all languages, with \textit{Python} showing the steepest rise, from a 30\% to a stable 80\% reached in 2023 (Figure \textbf{\ref{fig:python_monthly}}) . The other languages also follow a similar upward trend but not as steep.
		\item \textbf{Python's dominance over LLM-integrated development}, \textit{Python} accounts for over 260,000 repositories, compared to 8,639 for \textit{Java} and 5,511 for \textit{Go} in our combined dataset, making it two orders of magnitude more widely used for LLM applications.
		\item \textbf{OpenAI and Google SDKs are the most adopted among official libraries}, although third-party libraries remain popular across all languages. For \textit{Python} (Figure \textbf{\ref{fig:python_library}}), official SDKs show much stronger adoption than in \textit{Java} (\textbf{Figure \ref{fig:java_library}}) and \textit{Go} (Figure \textbf{\ref{fig:go_library}}), where third-party libraries remains the most utilized.
		\item \textbf{A small set of LLM models accounts for most real-world usage}, Across languages, the top 20\% most-frequent models constitute over 80\% of model references, with \textit{OpenAI} models dominating and \textit{Google} models appearing as secondary but significant contributors (Figures \textbf{\ref{fig:python_model} \ref{fig:java_model} \ref{fig:go_model}}).
		\item \textbf{Common usage patterns align with conversational, generative, and agentic workflows}, Based on keyword-based classification, LLMs in public repositories are primarily used for conversational agents, content generation, and multi-step agentic orchestration, with fewer repositories employing retrieval-augmented pipelines or domain-specific applications (Figures \textbf{\ref{fig:python_cat_prop} \ref{fig:java_cat_prop} \ref{fig:go_cat_prop}}).
		\item \textbf{Repository-level attributes show interesting correlations}, Spearman correlation analyses reveal distinct patterns across languages. In \textit{Python}, open issues correlate strongly with fork counts (Figure \textbf{\ref{fig:python_corr}}), while in \textit{Java} and \textbf{Go} stars correlate more with open issues (Figures \textbf{\ref{fig:java_corr} \ref{fig:go_corr}}). Statistical hypothesis testing further shows that \textit{Python} LLM repositories have generally fewer stars than \textit{Java} and \textit{Go} repositories, suggesting lower visibility or more experimental project character.
	\end{itemize}
	
	These findings paint a clear picture of the current LLM ecosystem where \textit{Python} is the central hub of LLM-integrated development, \textit{OpenAI} remains the dominant provider, and real-world usage tends to use a relatively narrow set of models and application patterns.
	
	The remainder of this paper is organized as follows:
	\begin{itemize}
		\item \textbf{Section 2, Related Work}: Presents the related work, positioning our study within the broader landscape of LLM-integrated software engineering research.
		\item \textbf{Section 3, Experiment Design \& Execution}: Details our experimental design, outlining the research questions and the metrics used to answer them, and describes the execution of the experiment, including data collection strategies for keyword-search, library-search, model-search, and repository attribute extraction.
		\item \textbf{Section 4, Data Processing \& Cleanup}: Discusses the preprocessing and cleaning steps applied to the dataset.
		\item \textbf{Section 5, Results}: Reports the results for each research question, covering popularity trends, language and SDK distribution, model usage analysis, usage patterns, and repository attribute correlations.
		\item \textbf{Section 6, Threats to Validity}: Describes the various threats that are present in this research and provides some suggestions on how to mitigate them.
		\item \textbf{Section 7, Discussion}: Provides a discussion and interpretation of the results, outlining limitations and implications for future research.
		\item \textbf{Section 8, Conclusion}: Concludes the study with a summary of our contributions and opportunities for follow-up investigations.
	\end{itemize}
	
	
	%------------------------------------------------
	
	\section{Related Work}
	
	The existing literature already includes numerous empirical studies covering a wide range of topics related to Large Language Models (LLMs), such as exploring new domains of application (e.g., assisting in peer review \autocite{zhou-etal-2024-llm}, code understanding \autocite{10.1145/3597503.3639187}, and LLM-as-a-judge scenarios \autocite{gu2025surveyllmasajudge}), as well as studies focused on improving their performance \autocite{NEURIPS2024-71c3451f}. 
	
	However, to our knowledge, there have not been many studies on the popularity of different LLM technologies in LLM-integrated software. An important study on the topic of LLM-integrated software \autocite{weber2024largelanguagemodelssoftware} attempts to establish new terminology, concepts, and methods for LLM-integrated application engineering. LLM-integrated application engineering refers to an emerging branch of software engineering that studies LLM-integrated applications, which are software systems that leverage LLMs to provide functionalities that would otherwise be infeasible or require substantial development effort to implement. To this end, the study proposes a structured framework for categorizing and analyzing LLM-integrated applications across various domains. 
	
	Another paper on the same topic \autocite{10.1145/3715007} explores LLM-integrated application engineering from the perspective of its challenges. These challenges span different areas, such as prompts, APIs, and plugins, and require developers to navigate unique methodologies and considerations specific to LLM application development. The authors crawled and analyzed approximately 30,000 relevant questions from popular OpenAI developer forums to build a taxonomy of challenges faced by LLM developers and summarized a set of findings and actionable insights. 
	
	In summary, we did not find important papers detailing the popularity of various LLM technologies through API usage, but we did find foundational papers on LLM-integrated application development.
	%------------------------------------------------
	\pagebreak
	\raggedbottom
	
	\section{Experiment Design And Execution}
	
	\subsection{Experiment Design}
	
	The goal of this study is to investigate the real-world adoption and usage patterns of Large Language Models (LLMs) in public software projects. To achieve this goal, we define the following research questions:
	
	\begin{enumerate}
		\item How has the popularity of LLM-integrated software evolved since the first announcement of ChatGPT in 2022?
		\item What is the most popular programming language for LLM-integrated software development?
		\item What is the most popular SDK for LLM-integrated software development?
		\item Which LLM models are most frequently used for inference in public GitHub projects?
		\item How are these models being utilized in practice within these repositories?
		\item How do different programming languages compare with each other in terms of LLM-project size, number of stars and other related attributes?
	\end{enumerate}
	
	To answer these questions, we define the following metrics and data collection strategies:
	\begin{itemize}
		\item \textbf{Popularity over time}
		We will track the presence of LLM-related keywords in repository descriptions, topics, or names among the most-starred public GitHub repositories. This will be done monthly from January 2022 to October 2025. We will calculate the proportion of repositories that match these criteria each month and plot the evolution over time. As result we will build a graph representing the evolution of the proportion of LLM-related repos in the given time interval.
		
		\item \textbf{Popular programming languages}  
		We will identify repositories that include SDK imports or model inference code and aggregate them by programming language. The language with the highest number of such repositories will be considered the most popular for LLM-integrated software development. This requires a method to query and retrieve all relevant repositories effectively (chunking).
		
		\item \textbf{Popular SDKs}  
		For each SDK, we will count the total number of repositories that include its import. The SDK with the highest occurrence will be identified as the most widely adopted.
		
		\item \textbf{Popular LLM models}  
		We will analyze the repositories to detect which models are referenced most frequently, based on model names or API calls for inference.
		
		\item \textbf{Usage patterns of LLMs}  
		We will perform a qualitative analysis of how models are used in these repositories, categorizing usage into common patterns: Conversational, Generation task based, Retrieval-Augmented, Agentic Orchestration, Multimodal or Domain-Specific.
		
		\item \textbf{Repository attribute patterns}
		By randomly sampling five hundred repositories and querying the GitHub endpoint for specific metadata, we hope to compare various attributes of LLM-integrated software in different languages like size or the number of stars. We will thus produce a table expressing the correlation between these different stats and apply some reasonable statistical tests to gather further insights.
		
	\end{itemize}
	
	By systematically measuring these metrics, we aim to quantify the adoption trends, identify the most popular development practices, and understand how LLMs are integrated into real-world software projects.
	
	\subsection{Experiment Execution}
	The experiment will mainly consist of four phases:
	\begin{enumerate}
		\item \textbf{Keyword-Search} \textendash We will use the repository search endpoint to keep track of the popularity metric.
		\item \textbf{Library-Search} \textendash We will use the code search endpoint to retrieve all occurrences of repositories with corresponding imports.
		\item \textbf{Model-Search} \textendash We will use the code search endpoint to retrieve all occurrences of repositories with corresponding reference to model names.
		\item \textbf{Attribute-Search} \textendash Starting from the complete list of all LLM-related repositories obtained previously, we sample and then query the repository endpoint for detailed repo attributes. 
	\end{enumerate}
	We subsequently report the considerations we made for each phase, the limits, the execution processes, and the preliminary results.
	
	\subsubsection{Data Collection for Keyword-Search}
	To have a rough estimate of the popularity of LLM projects in GitHub for various languages, we first did a monthly analysis on the one hundred most popular repositories created that month and memorized useful metadata like number of stars, forks, topics, description, etc. This analysis went from January 2022, the year in which ChatGPT was launched, to October 2025, when this study began. The code needs a valid GitHub token for authentication, which can be created from the settings page in your GitHub account. When run, the script (repo\_search.py) queries GitHub for the most popular repositories created each month, retrieving interesting metadata and writing to a JSON file that serves as storage. The data collection process for this step did not take much time since with one request, we could collect all the repos we needed for a month.
	
	\subsubsection{Setting up Library-Search}
	The first step is to gather the names and URLs of the GitHub repositories that import libraries of the chosen LLM providers, but we need some boundaries to the domain of the repositories; otherwise, the scale of data to analyze would be too much and would hinder the research. We settled on size boundaries. The domain of this study comprises all the GitHub repositories with files whose size is less than 5MB, plus language constraints. Furthermore, we decided to focus on the most mainstream, accessible, and easily implementable LLMs; this was done because not every LLM has an official or a reputable and endorsed third-party SDK, while other LLMs are not easily accessible, with their documentation and models being put behind a paywall. So for these reasons, we decided that our study will focus on \textit{OpenAI}, \textit{Gemini}, \textit{Anthropic}, \textit{Minstral}, \textit{xAI}, and \textit{Meta}.
	The former two LLMs are only counted for specific languages; for example, \textit{xAI} only has an official SDK for \textit{Python} and none for the other languages. More importantly, there aren't any reputable third-party libraries, neither for \textit{Java} nor \textit{Go}, and so we have analyzed the usage of \textit{xAI} only on \textit{Python} files; the same goes for \textit{Meta}. 
	
	Since the GitHub Rest API does not support code search based on regular expressions, we decided to do a word list-based research. The word list was built for each language by looking at its import syntax and creating a list of every possible way to import the LLM client. For example, in the case of Python, there are two major import syntaxes for the OpenAI client:
	\begin{itemize}
		\item \textbf{import openai.OpenAI as alias}
		\item \textbf{from openai import Openai}
	\end{itemize}
	Since GitHub code search does not distinguish between upper and lowercase letters, we can cover both cases by searching for "import openai". Lastly, for the data collection script to work, we need a Python dictionary that associates each word with a label that will be applied to the repository. This dictionary can be easily built manually by associating each library keyword with the providing company.
	
	\subsubsection{Setting up Model-Search}
	Other than the LLM library used in a project, we are also interested in the specific LLM model that the LLM-integrated application is interfacing with. To do this, we create a starting list of the most important models for each provider by visiting the models and pricing page on their official site. This list is then enriched by combining it with the results of an API call querying the complete list of available models for each provider (when the list operation is available). model-search, just like library-search, uses the same language scope for code mining. This means only for \textit{Python}, \textit{Java} and \textit{Go} files and only for models that can be used by official SDK provided by  \textit{OpenAI}, \textit{Gemini}, \textit{Anthropic}, \textit{Minstral}, \textit{xAI}, or \textit{Meta}. 
	
	Just like for Library-Search, Model-Search is implemented as code search based on word matching. To avoid being blocked by models with very short names (like o1), we search for \textbackslash"word\textbackslash" instead of simply searching for word in a script file. The keyword dictionary, in the case of Model-Search, cannot be simply created manually, given its higher length. We will need to build it in a separate script by simply associating each model keyword with the name of the model.
	
	\subsubsection{Data Collection for Library-Search and Model-Search}
	The script (mine.py) employs a dynamic chunking strategy based on size to be able to retrieve all occurrences of files satisfying certain queries. This query can be based on model names for Model-Search or on imports for Library-Search. A dedicated part of the script is used to manage rate-limiting from GitHub by pausing the execution each time we hit it until the next reset time. We query the endpoint a page at a time, with each page containing one hundred items, until we finish all results. If we get one thousand or more results, we stop and launch an exception because we could not retrieve all occurrences of the searched pattern. Lastly, based on the number of results, we adjust the size interval so that we can go faster when the queries are mostly empty, while not reaching the thousand cap when the results are dense. 
	
	To complete the data collection step, we need to run the same function (collect\_repo\_by\_language) twice for each language: in the first run, we search for library imports, while in the second run for model names. Given the objective of retrieving everything and the long list of models to search for, this data collection step was the most time-consuming of all, needing almost a week to complete.
	
	\subsubsection{Data Collection for Attribute-Search}
	Starting from the list of collected repos we randomly sample five hundred repositories and query the GitHub endpoint for attributes of interest. The script is quite simple, and it follows more or less the same logic outlined by Keyword-Search. The github\_get function allows for managing authentication errors and rate-limiting while repeating the request in case of an unstable internet connection. The execution process did not take much time, and the only error we encountered was the 404 not found error. This error is probably caused by deleted projects or projects that have been made private since the execution of Model and Library-Search. To avoid this error, we just repeat the sampling process multiple times until we reach a run without the error, which should not take too many runs since this error is rare (you encounter one such repo in five hundred).
	
	\subsection{Data Preprocessing and Cleanup}
	In this section, we discuss how we preprocessed and cleaned up the collected data from the previous steps.
	
	\subsubsection{Data Preprocessing for Keyword-Search}
	The collected Data for Keyword-search is already in a format that can be worked with. Since we only need to use the topics, description, and name fields, even if some of them are null (description or name), the wordlist matching process would not change. The collected data is memorized in a JSON file as a list of dicts containing various useful repository metadata. This JSON file could then be loaded into a Python list and worked with.
	
	\subsubsection{Data Preprocessing for Library-Search and Model-Search}
	The data collected by the mining script is stored in a JSON file format, containing a list of dictionaries. The dicts contain the name of the repository, URL, HTML that is a link to the human-readable page of the project, and a label dict. The label dict contains, for each label (the keyword that was found), a list of file paths. Each file path represents the location where the keyword was found. We are interested in knowing the relationship between model data and the library data, so we need to first combine the two data dictionaries before any further actions.
	
	The combine function combines the two JSON files to derive data about Model-Library correlation. First, we consider the repositories appearing in both lists: these repositories contain both the library import and the model reference, and as such, we can label them with both. In the case where the import and the model reference are in the same file (we also remember the filepaths), we can pair the two labels up. If we only found the import in a repository, we label it with the name of the provider, while if we only found a reference to a model, we label it with unknown library. In this way, we obtained a list of repositories with informative labels that could then be used in the analysis phase. 
	
	This step is done by running the combine function in the combine.py file. A special dict called model-library map is required for the script to be able to match models to the correct SDK. We built this Python dictionary starting from the previous list of models by creating a key for each model and associating it with an empty list of strings, which is then updated by appending possible parent SDKs. In this way, only correct SDK-Model matchings will be created in the final JSON file.
	
	\subsubsection{Data Preprocessing for Attribute-Search}
	Like Keyword-Search, the data collected with Attribute-Search can already be worked with without much hindrance. The data is collected as a list of Python dictionaries, containing for each attribute-key a value that can be null. Everything is then stored in a JSON file, like always.
	%------------------------------------------------
	
	\section{Results}
	In this section of the report, we show the results of our study and our answer to each of the initial questions concerning the real-world adoption and usage of LLM technology. 
	
	\subsection{Popularity over time}
	We respond to this question by analyzing the results collected from Keyword-Search. We analyzed the name, the description, and the topics for each repository and classified a repository as LLM-related if it contained at least a word from our list of keywords. We then just show the proportion of LLM-related projects grouped by the month of creation of the project.
	\begin{figure}[!h]
		\includegraphics[width=1\linewidth]{Figures/python\_llm\_fraction.png}
		\caption{monthly proportion of Public Python projects with LLM-related terms in their description, name or topics}
		\label{fig:python_monthly}
	\end{figure}
	In Figure \textbf{\ref{fig:python_monthly}} we can observe that the proportion of LLM-related terms in the most starred public GitHub Python projects, started from a level of 30\% in 2022 and increased until a stable 80\% in the first half of 2023. We can see that a majority of the most popular new Python projects were about AI and LLMs in the last three years.
	
	\pagebreak
	\raggedbottom
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/java\_llm\_fraction.png}
		\caption{monthly proportion of Public Java projects with LLM-related terms in their description, name or topics}
		\label{fig:java_monthly}
	\end{figure}

	For Java, in Figure \textbf{\ref{fig:java_monthly}}, we observe a lower incidence of LLM-related terms appearing in the most starred recent projects, however we can still see an evident increase in popularity in the last two years.
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/go\_llm\_fraction.png}
		\caption{monthly proportion of Public Go projects with LLM-related terms in their description, name or topics}
		\label{fig:go_monthly}
	\end{figure}

	For Golang projects, Figures \textbf{\ref{fig:go_monthly}}, we see higher proportions of LLM-related projects and a similar peak in the may of 2025.
	
	\subsection{Popular programming languages}
	To respond to this question we just compare the number of repositories we collected for each language in Model-Search and Library-Search. 
	\begin{enumerate}
		\item Number of repositories in the combined JSON file for Python: 262581
		\item Number of repositories in the combined JSON file for Java: 8639
		\item Number of repositories in the combined JSON file for Golang: 5511
	\end{enumerate}
	From the collected data we can see that Python is two order of magnitude more used for LLM-integrated application development than the other two analyzed languages.
	
	\subsection{Popular SDKs}
	Each of the selected provider of LLM technology offers its own SDK to interface smoothly with its services. Like what we have already said in the experiment design section, official SDKs exist only for the most popular languages. We have built three graphs, one for each of the three languages we collected data for (Figures \textbf{\ref{fig:python_library}}, \textbf{\ref{fig:java_library}}, \textbf{\ref{fig:go_library}}). Each graph will show the number of repositories that were found with the specific import statement that links it to the corresponding official SDK. Lastly, if a model reference was fopund in a repository with no known import pattern then the repository will be linked to 'other', in other words to a third party SDK.
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_library\_usage\_python.png}
		\caption{Number of repositories found with code search with the imports of the relative SDK in Python}
		\label{fig:python_library}
	\end{figure}
	
		\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_library\_usage\_java.png}
		\caption{Number of repositories found with code search with the imports of the relative SDK in Java}
		\label{fig:java_library}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_library\_usage\_go.png}
		\caption{Number of repositories found with code search with the imports of the relative SDK in Golang}
		\label{fig:go_library}
	\end{figure}
	
	These three graphs allow us to conclude that third-party library remain very popular in all the languages, while among official SDKs, \textit{OpenAI} and \textit{Google} remain dominant, with \textit{xAI} being the less used given that it does not have free models.
	
	\raggedbottom
	\pagebreak
	
	\subsection{Popular LLM models}
	Using the list of all models we derived previously with the list-model functionality for each provider, we count the number of references for each model in our dataset. In all three languages we see the trend of the 20\% top most used models making up more than 80\% of total model usage, suggesting that most developer use the most accessible and/or popular model (Figures \textbf{\ref{fig:python_model}}, \textbf{\ref{fig:java_model}}, \textbf{\ref{fig:go_model}}).
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_model\_usage\_python.png}
		\caption{Number of references for each model in decreasing order for Python repositories}
		\label{fig:python_model}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_model\_usage\_java.png}
		\caption{Number of references for each model in decreasing order for Java repositories}
		\label{fig:java_model}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/llm\_model\_usage\_go.png}
		\caption{Number of references for each model in decreasing order for Golang repositories}
		\label{fig:go_model}
	\end{figure}
	
	In particular, for the twenty most referenced models, we conducted a deeper analysis to examine which libraries they were used in association with. Our method does not, in fact, guarantee that if an import statement for a library and a reference to a model appear in the same repository, or even in the same file, the model is actually used through that library. It only underlines association. Now we can show the graphs we made to show the association of various models with the five official SDKs we are considering (Figures \textbf{\ref{fig:python_model_prop}}, \textbf{\ref{fig:java_model_prop}}, \textbf{\ref{fig:go_model_prop}}).
	
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/top\_models\_prop\_python.png}
		\caption{Proportions of models found within the same file or repository of an import for Python projects}
		\label{fig:python_model_prop}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/top\_models\_prop\_java.png}
		\caption{Proportions of models found within the same file or repository of an import for Java projects}
		\label{fig:java_model_prop}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/top\_models\_prop\_go.png}
		\caption{Proportions of models found within the same file or repository of an import for Golang projects}
		\label{fig:go_model_prop}
	\end{figure}
	
	We see that our script was not able to associate most model references to a known official SDK in \textit{Java} and \textit{Go}, but was able to do so for the \textit{Python} dataset. This may be caused by the higher popularity of official SDKs in Python and the higher popularity of \textit{Python} itself for LLM-integrated software development. The most popular models do not change much between languages and are mostly dominated by \textit{OpenAI} models with the occasional \textit{Google} model. We can also observe that in correspondence of \textit{Google} models we can see a significant proportion of association with the \textit{Gemini} SDK, confirming the correctness of our method.
	
	\raggedbottom
	\pagebreak
	
	\subsection{Usage patterns of LLMs}
	Let us first discuss the classification method we decided to settle on:
	\begin{enumerate}
		\item \textbf{Conversational Agents} --- Used for direct human-LLM interaction in dialog form (e.g., chatbots, virtual assistants, helpdesk systems).
		\item \textbf{Generation Agents} --- Used to produce text, code, or other content using LLMs (e.g., article writing, code completion, story generation).
		\item \textbf{Retrieval-Augmented} --- Uses external knowledge sources to enhance LLM outputs and improve correctness (e.g., document question answering, knowledge-base assisted generation).
		\item \textbf{Agentic Orchestration} --- Used when the LLM drives multi-step actions, calls APIs, or interacts with other software components (e.g., workflow automation, multi-step task agents, Auto-GPT style agents).
		\item \textbf{Multimodal} --- Handles multiple data types, such as text, images, audio, and video (e.g., image captioning, audio transcription, video summarization).
		\item \textbf{Domain-Specific} --- Tailored LLM applications designed for a particular field or domain (e.g., legal AI assistants, medical Q\&A systems, financial analysis tools).
	\end{enumerate}
	We based our classification method by matching the description, topics and name of the repository against a list of probable keywords for each category. This keyword matching method is not very efficient and leaves a lot of the repositories unclassified. Nevertheless we think that this method is a reliable way to classify the repositories based on purpose.
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/python\_llm\_category\_proportions.png}
		\caption{Proportions of various LLM-integrated software category for Python}
		\label{fig:python_cat_prop}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/java\_llm\_category\_proportions.png}
		\caption{Proportions of various LLM-integrated software category for Java}
		\label{fig:java_cat_prop}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/go\_llm\_category\_proportions.png}
		\caption{Proportions of various LLM-integrated software category for Golang}
		\label{fig:go_cat_prop}
	\end{figure}
	\raggedbottom
	\pagebreak
	
	We can observe from the bar charts (Figures \textbf{\ref{fig:python_cat_prop}}, \textbf{\ref{fig:java_cat_prop}}, \textbf{\ref{fig:go_cat_prop}}) we built that between the classified repositories most llm are used as conversational agents, for generation tasks or for agentic orchestration.
	
	\subsection{Repository attribute patterns}
	Lastly, we try to statistically analyze the attributes of an average LLM-integrated software on GitHub. Let us start by checking how correlated are the numeric attributes of the LLM-using GitHub repositories by computing the Spearman's rho for each pair.
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/table\_corr\_python.png}
		\caption{Spearman's Correlation Heatmap for Python}
		\label{fig:python_corr}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/table\_corr\_go.png}
		\caption{Spearman's Correlation Heatmap for Go}
		\label{fig:go_corr}
	\end{figure}
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/table\_corr\_java.png}
		\caption{Spearman's Correlation Heatmap for Java}
		\label{fig:java_corr}
	\end{figure}
	
	The Spearman's correlation matrices for the \textit{Java} and \textit{Go} datasets are very similar with each-other (Figures \textbf{\ref{fig:java_corr}}, \textbf{\ref{fig:go_corr}}), while the one built from the \textit{Python} dataset is quite different (Figure \textbf{\ref{fig:python_corr}}). The biggest observable difference is that for \textit{Python} projects the number of open issues correlates the most with the network count (number of forks) while in the other two languages it correlates the most with the number of stars of a project. This suggests that \textit{Python} LLM projects are less visible, with activity driven by contributor engagement rather than popularity. This may be due to \textit{Python}’s low barrier to entry, leading to many experimental projects with few stars. Let us Hypothesis test this.\\
	First test (Python vs Java): 
	\begin{itemize}
		\item $H_0$ --- \textbf{Null Hypothesis}\\
		The number of stars in \textit{Python} LLM Repositories has the same distribution as the number of stars in \textit{Java} LLM Repositories.
		\item $H_1$ --- \textbf{Alternative Hypothesis}\\
		The number of stars in \textit{Python} LLM Repositories is stochastically less than the number of stars in \textit{Java} LLM Repositories.
	\end{itemize}
	Second test (\textit{Python} vs \textit{Go}): 
	\begin{itemize}
		\item $H_0$ --- \textbf{Null Hypothesis}\\
		The number of stars in \textit{Python} LLM Repositories has the same distribution as the number of stars in \textit{Go} LLM Repositories.
		\item $H_1$ --- \textbf{Alternative Hypothesis}\\
		The number of stars in \textit{Python} LLM Repositories is stochastically less than the number of stars in \textit{Go} LLM Repositories.
	\end{itemize}
	We randomly sampled 500 repositories for all three languages and took record of the star number for each repository. We then applied the Anderson–Darling test to assess whether the repository size and star count data follow a normal distribution. This step is fundamental, as it determines the appropriate hypothesis testing method to use (parametric or non-parametric). The results visible in Figures \textbf{\ref{fig:anderson_size}} and \textbf{\ref{fig:anderson_stars}}, indicate that neither distribution is normally distributed. Consequently, we employed a non-parametric hypothesis testing method, specifically the Mann–Whitney U test.
	
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/anderson\_darling\_size\_test.png}
		\caption{Results for the Anderson-Darling test for the size data.}
		\label{fig:anderson_size}
	\end{figure}
	\begin{figure}[h!]
		\includegraphics[width=1\linewidth]{Figures/anderson\_darling\_stars\_test.png}
		\caption{Results for the Anderson-Darling test for the stars data.}
		\label{fig:anderson_stars}
	\end{figure}

	Basically, the Mann-Whitney U test checks whether the distributions underlying the two input samples are the same. In out specific case we apply the one-sided test checking that the distribution of star numbers in Python repositories are stochastically less than in the other two languages. Stochastically less means that values from X tend to be smaller than values from Y in a distribution-wide sense. Formally X is stochastically less than Y means that:
	$P(X>u)\le P(Y>u)$ \hspace*{0.2cm} $\forall u$.
	
	\raggedbottom
	\pagebreak
	
	The results of applying the test were the following, considering a significance level of $\alpha=0.05$:
	\begin{itemize}
		\item Python vs Java ---\\
		$U-statistic=118646.5000$\\
		$p-value=0.047242$\\
		$p-value<\alpha$, we reject the null hypothesis.
		\item Python vs Go ---\\
		$U-statistic=108120.0000$\\
		$p-value= 0.000008$\\
		$p-value<\alpha$, we reject the null hypothesis.
	\end{itemize}
	Both p-values are lower than the significance level $\alpha=0.05$ leading us to reject the null hypotheses $H_0$ for both comparisons. In other words, there is enough evidence to support that the distribution of number of stars for \textit{Python} LLM projects are stochastically less than those of \textit{Java} and \textit{Go} LLM projects. Thus, \textit{Python} LLM repositories have statistically fewer stars, supporting the observation of lower visibility compared to \textit{Java} and \textit{Go} projects. A natural thing we can derive is that since \textit{Python} repositories are generally less visible and more experimental, their size may be smaller than their corresponding \textit{Java} and \textit{Go} projects. 
	
	The paper \autocite{zhang2009distributionprogramsizesimplications} supports a log-normal distribution of program sizes together with the Anderson-Darling tests we did earlier, so we can use the same procedure we applied for the number of stars.
	First size test (Python vs Java):
	\begin{itemize}
		\item $H_0$ --- Null Hypothesis\\
		The sizes of Python and Java LLM repositories have the same distribution.
		\item $H_1$ --- Alternative Hypothesis\\
		The distribution of sizes in Python repositories are stochastically less than in Java repositories
	\end{itemize}
	Second size test (Python vs Golang): 
	\begin{itemize}
		\item $H_0$ --- Null Hypothesis\\
		The sizes of Python and Golang LLM repositories have the same distribution.
		\item $H_1$ --- Alternative Hypothesis\\
		The distribution of sizes in Python repositories are stochastically less than in Golang repositories
	\end{itemize}
	U-statistic and p-value for the two Mann–Whitney U, considering a significance level of $\alpha=0.05$:
	\begin{itemize}
		\item Python vs Java ---\\
		$U-statistic=105600.0000$\\
		$p-value=0.000011$\\
		$p-value<\alpha$, we reject the null hypothesis.
		\item Python vs Go ---\\
		$U-statistic=114880.0000$\\
		$p-value=0.013346$\\
		$p-value<\alpha$, we reject the null hypothesis.
	\end{itemize}
	The statistical tests once again supports our reasoning. These findings support the hypothesis that Python LLM projects tend to be less prominent and smaller in scope. However, identifying the underlying causes, such as ecosystem norms, project maturity, or contributor profiles, requires further analysis.
	
	%------------------------------------------------
	
	\section{Threats to validity}
	When interpreting the results of this study, several potential threats to validity must be considered:
	\begin{itemize}
		\item External Validity:
		\begin{itemize}
			\item Our analysis focuses exclusively on public GitHub repositories and three programming languages: \textit{Python}, \textit{Java}, and \textit{Go}.
			\item Projects in private repositories or written in other languages may exhibit different patterns of LLM usage, limiting generalizability.
			\item Additionally, we only consider LLMs with official SDKs from six providers; thus, findings may not generalize to niche or proprietary LLMs.
		\end{itemize}
		\item Internal Validity:
		\begin{itemize}
			\item The reliance on keyword and import-based code searches may miss repositories that use unconventional naming, dynamic imports, or indirect API wrappers.
			\item Rate-limiting and the GitHub search API cap (1000 results per query) might have caused incomplete data retrieval for highly popular SDKs or models.
		\end{itemize}
		\item Construct Validity:
		\begin{itemize}
			\item The classification of repositories into usage patterns (Conversational, Generation, etc.) is based on keyword matching in topics, description, and repository names, this method may misclassify or leave many repositories unclassified, potentially affecting the accuracy of usage pattern analysis.
			\item Similarly, model-library association is inferred based on co-occurrence within a file or repository, which may not reflect actual usage.
		\end{itemize}
		\item Conclusion Validity:
		\begin{itemize}
			\item Statistical analyses rely on random samples of 500 repositories for hypothesis tests, which are only sampled one time.
			\item Correlations observed between repository attributes are descriptive and do not imply causation.
		\end{itemize}
	\end{itemize}
	\section{Discussion}
	Our results provide several insights into the real-world adoption of LLMs in GitHub repositories. \textit{Python} dominates LLM-integrated development, with two orders of magnitude more repositories than \textit{Java} or \textit{Go}, while \textit{OpenAI} SDK and models are the most widely adopted across all languages. Other SDKs like \textit{xAI} have minimal presence, likely due to limited accessibility. The usage patterns are primarily Conversational, Generation, and Agentic Orchestration, reflecting common application scenarios like chatbots, content generation, and workflow automation. We lastly concluded, through statistical analysis, that Python repositories tend to have fewer stars and smaller size, suggesting many experimental or prototype projects, whereas \textit{Go} and \textit{Java} projects are larger and more visible.
	
	Our research is useful for LLM-integrated software developers: it helps in selecting LLM models and SDKs with the largest community support. For example, using \textit{Python} with \textit{OpenAI} SDK maximizes compatibility and access to widely tested models. As for lessons learned wefound that SDK accessibility influences greatly the language choice, while keyword-based classification is effective for high-level categorization but could be improved with NLP-based semantic analysis for more accurate usage pattern identification.
	
	\section{Conclusions}
	
	This study offers a comprehensive observational analysis of LLM adoption in public GitHub repositories.
	
	\subsection{Main Contributions}
	\begin{enumerate}
		\item Quantified the evolution of LLM-integrated software popularity over 2022–2025, showing rapid adoption, especially in \textit{Python}.
		\item Identified the most popular SDKs (\textit{OpenAI}, \textit{Google}) and models, highlighting the 80/20 pattern where a few models dominate usage.
		\item Classified usage patterns across repositories, providing actionable insights for developers and researchers.
		\item Conducted statistical analysis of repository attributes, revealing differences in visibility and project scale across languages.
	\end{enumerate}
	
	\subsection{Future Work}
	\begin{itemize}
		\item Extend the study to \textbf{private repositories} or \textbf{other programming languages} to improve external validity.
		\item Enhance usage pattern classification using \textbf{NLP-based semantic analysis} or \textbf{machine learning} to reduce misclassification.
		\item Investigate \textbf{causal relationships} between SDK choice, project visibility, and adoption of LLMs.
		\item Explore \textbf{multi-model projects} and \textbf{cross-SDK integrations} to better understand complex LLM application architectures.
	\end{itemize}
	
	%----------------------------------------------------------------------------------------
	%	 REFERENCES
	%----------------------------------------------------------------------------------------
	
	\printbibliography % Output the bibliography
	
	%----------------------------------------------------------------------------------------
	
\end{document}
