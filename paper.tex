%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	a4paper, % Paper size, use either a4paper or letterpaper
	10pt, % Default font size, can also use 11pt or 12pt, although this is not recommended
	unnumberedsections, % Comment to enable section numbering
	twoside, % Two side traditional mode where headers and footers change between odd and even pages, comment this option to make them fixed
]{LTJournalArticle}

\addbibresource{paper.bib} % BibLaTeX bibliography file

\runninghead{Usage of LLMs in Github repos} % A shortened article title to appear in the running head, leave this command empty for no running head

\footertext{} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Observational Study on usage of Large Language Models in public Github repositories} % Article title, use manual lines breaks (\\) to beautify the layout

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{%
	Davide Zhang\textsuperscript{1} and Roberto Magrini\textsuperscript{1}
	\thanks{Emails: \href{mailto:davide.zhang@edu.unifi.it}{davide.zhang@edu.unifi.it} \href{mailto:roberto.magrini@edu.unifi.it}{roberto.magrini@edu.unifi.it}}
}

% Affiliations are output in the \date{} command
\date{\footnotesize\textsuperscript{\textbf{1}}Scuola di Scienze Matematiche Fisiche e Naturali, UNIVERSITÀ DEGLI STUDI FIRENZE}

% Full-width abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent The current landscape of software engineering is increasingly shaped by the adoption of Large Language Models (LLMs), both as tools that support the development process and as integrated components of final applications. Understanding how and when developers apply these models is therefore essential. This study surveys public GitHub repositories to identify the LLM technologies they employ, if any, and to collect additional metrics that support a broader understanding of the popularity and distribution of different LLM technologies, their areas of application, and their correlation with other project characteristics. We will collect data using GitHub's repository and code search functionality through the REST APIs. To have a first understanding of the popularity of LLM technology throughout the years, we collected metadata about the most popular GitHub repositories for each month from 2022 until October 2025. Then, to have a more detailed view, we used code mining to collect data regarding two aspects of a repository relative to the use of LLM tech: the SDK and the model used. The data will be collected through two mining sessions (one for the SDK and one for the model) by code search up to files of 5MB in size. The data is then combined to identify frequent SDK-model pairs and other interesting metrics of analysis.   
    
		% TO DO - results and conclusion.
	\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the title section

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

Our objective is to thoroughly analyze the usage and prominence of LLMs in GitHub repositories. Many different languages can be used to do this analysis, but we decided to only consider the following languages:

\begin{itemize}
    \item \textbf{Python}, the most popular language for LLM inference through programmatic API.
    \item \textbf{Java}, a popular high-level language for the development of web applications.
    \item \textbf{Go}, similar to Java but with a simpler syntax.
\end{itemize}

We also decided to only consider the official SDK provided by the following makers:

\begin{itemize}
    \item \textbf{OpenAI} 
    \item \textbf{Anthropic}
    \item \textbf{Mistral}
    \item \textbf{Google}
    \item \textbf{xAI}
    \item \textbf{Meta}
\end{itemize}

The list of available models for each maker was obtained by visiting the official site and combining results there with the return of a query to the API of the maker when the call was free.

To reach our goal, we first analyzed for each language the evolution of the percentage of repositories containing LLM-adjacent words in their topic, description, or title. From the data we collected, we performed several analyses, such as which language is the most popular for the development of LLM-integrated applications or which repository metrics are more correlated. Starting from this analysis, we conducted a deeper research on public repositories on GitHub by using code search to retrieve all instances of relevant SDK import or model reference. The collected data is combined to obtain SDK-model pairs that can be analyzed and used to derive insights for our study.

% TODO:
%   ADD THE MAIN RESULTS OF THE STUDY
%   ADD MAIN CONTRIBUTION (HOW TO USE THE PAPER)
%   ADD THE STRUCTURE OF THE PAPER


%------------------------------------------------

\section{Related Work}

The existing literature already includes numerous empirical studies covering a wide range of topics related to Large Language Models (LLMs), such as exploring new domains of application (e.g., assisting in peer review \autocite{zhou-etal-2024-llm}, code understanding \autocite{10.1145/3597503.3639187}, and LLM-as-a-judge scenarios \autocite{gu2025surveyllmasajudge}), as well as studies focused on improving their performance \autocite{NEURIPS2024-71c3451f}. 

However, to our knowledge, there have not been many studies on the popularity of different LLM technologies in LLM-integrated software. An important study on the topic of LLM-integrated software \autocite{weber2024largelanguagemodelssoftware} attempts to establish new terminology, concepts, and methods for LLM-integrated application engineering. LLM-integrated application engineering refers to an emerging branch of software engineering that studies LLM-integrated applications, which are software systems that leverage LLMs to provide functionalities that would otherwise be infeasible or require substantial development effort to implement. To this end, the study proposes a structured framework for categorizing and analyzing LLM-integrated applications across various domains. 

Another paper on the same topic \autocite{10.1145/3715007} explores LLM-integrated application engineering from the perspective of its challenges. These challenges span different areas, such as prompts, APIs, and plugins, and require developers to navigate unique methodologies and considerations specific to LLM application development. The authors crawled and analyzed approximately 30,000 relevant questions from popular OpenAI developer forums to build a taxonomy of challenges faced by LLM developers and summarized a set of findings and actionable insights. 

In summary, we did not find important papers detailing the popularity of various LLM technologies through API usage, but we did find foundational papers on LLM-integrated application development.
%------------------------------------------------

\section{Experiment Design And Execution}

\subsection{Experiment Design}

The goal of this study is to investigate the real-world adoption and usage patterns of Large Language Models (LLMs) in public software projects. To achieve this goal, we define the following research questions:

\begin{enumerate}
\item How has the popularity of LLM-integrated software evolved since the first announcement of ChatGPT in 2022?
\item What is the most popular programming language for LLM-integrated software development?
\item What is the most popular SDK for LLM-integrated software development?
\item Which LLM models are most frequently used for inference in public GitHub projects?
\item How are these models being utilized in practice within these repositories?
\item What is the average size, number of stars, forks etc. for a LLM-integrated application and how are they correlated?
\end{enumerate}

To answer these questions, we define the following metrics and data collection strategies:
\begin{itemize}
\item \textbf{Popularity over time}
We will track the presence of LLM-related keywords in repository descriptions, topics, or names among the most-starred public GitHub repositories. This will be done monthly from January 2022 to October 2025. We will calculate the proportion of repositories that match these criteria each month and plot the evolution over time. As result we will build a graph representing the evolution of the proportion of LLM-related repos in the given time interval.

\item \textbf{Popular programming languages}  
We will identify repositories that include SDK imports or model inference code and aggregate them by programming language. The language with the highest number of such repositories will be considered the most popular for LLM-integrated software development. This requires a method to query and retrieve all relevant repositories effectively (chunking).

\item \textbf{Popular SDKs}  
For each SDK, we will count the total number of repositories that include its import. The SDK with the highest occurrence will be identified as the most widely adopted.

\item \textbf{Most used LLM models}  
We will analyze the repositories to detect which models are referenced most frequently, based on model names or API calls for inference.

\item \textbf{Usage patterns of LLMs}  
We will perform a qualitative analysis of how models are used in these repositories, categorizing usage into common patterns: Chatbots, Retrieval‑Augmented Generation (RAG), Code generation, Text generation, Agents or Multimodal.

\item \textbf{Repository Attribute Patterns}
By randomly sampling five hundred repositories and querying the GitHub endpoint for specific metadata, we hope to estimate through statistical methods the mean of various attributes of LLM-integrated sofwtare like its size or its number of stars. We will lastly produce a table expressing the correlation between these different stats.

\end{itemize}

By systematically measuring these metrics, we aim to quantify the adoption trends, identify the most popular development practices, and understand how LLMs are integrated into real-world software projects.

\subsection{Experiment Execution}
The experiment will mainly consist of four phases:
\begin{enumerate}
    \item \textbf{Keyword-Search} \textendash We will use the repository search endpoint to keep track of the popularity metric.
    \item \textbf{Library-Search} \textendash We will use the code search endpoint to retrieve all occurrences of repositories with corresponding imports.
    \item \textbf{Model-Search} \textendash We will use the code search endpoint to retrieve all occurrences of repositories with corresponding reference to model names.
    \item \textbf{Attribute-Search} \textendash Starting from the complete list of all LLM-related repositories obtained previously, we sample and then query the repository endpoint for detailed repo attributes. 
\end{enumerate}
We subsequently report the considerations we made for each phase, the limits, the execution processes and the preliminar results.

\subsubsection{Data Collection for Keyword-Search}
To have a rough estimate of the popularity of LLM projects in GitHub for various languages, we first did a monthly analysis on the one hundred most popular repositories created that month and memorized useful metadata like number of stars, forks, topics, description, etc. This analysis went from January 2022, the year in which ChatGPT was launched, to October 2025, when this study began. The code needs a valid GitHub token for authentication, which can be created from the settings page in your GitHub account. When run, the script (repo\_search.py) queries GitHub for the most popular repositories created each month, retrieving interesting metadata and writing to a JSON file that serves as storage. The data collection process for this step did not take much time since with one request we could collect all repos we need for a month.

\subsubsection{Setting up Library-Search}
The first step is to gather the names and URLs of the GitHub repositories that import libraries of the chosen LLM providers, but we need some boundaries to the domain of the repositories; otherwise, the scale of data to analyze would be too much and would hinder the research. We settled on size boundaries. The domain of this study comprises all the GitHub repositories with files whose size is less than 5MB, plus language constraints. Furthermore, we decided to focus on the most mainstream, accessible, and easily implementable LLMs; this was done because not every LLM has an official or a reputable and endorsed third-party SDK, while other LLMs are not easily accessible, with their documentation and models being put behind a paywall. So for these reasons, we decided that our study will focus on \textit{OpenAI}, \textit{Gemini}, \textit{Anthropic}, \textit{Minstral}, \textit{xAI}, and \textit{Meta}.
The former two LLMs are only counted for specific languages; for example, \textit{xAI} only has an official SDK for \textit{Python} and none for the other languages. More importantly, there aren't any reputable third-party libraries, neither for \textit{Java} nor \textit{Go}, and so we have analyzed the usage of \textit{xAI} only on \textit{Python} files; the same goes for \textit{Meta}. 

Since the Github Rest API do not support code search based on regular expressions, we decided to do a word list based research. the word list were built for each language by looking at its import syntax and creating a list of every possible way to import the LLM client. For example, in case of Python, there are two major import syntaxes for the openai client:
\begin{itemize}
    \item \textbf{import openai.Openai as alias}
    \item \textbf{from openai import Openai}
\end{itemize}
since GitHub code search does not distinguish between upper and lowercase letters we can cover both cases by searching for "import openai". Lastly, for the data collection script to work, we need a python dictionary that associates each word with a label that will be applied to the repository. This dictionary can be easily built manually by associating each library keyword with the providing company.

\subsubsection{Setting up Model-Search}
Other than the LLM library used in a project, we are also interested in the specific LLM model that the LLM-integrated application is interfacing with. To do this, we create a starting list of the most important models for each provider by visiting the models and pricing page on their official site. This list is then enriched by combining it with the results of an API call querying the complete list of available models for each provider (when the list operation is available). model-search, just like library-search, uses the same language scope for code mining. This means only for \textit{Python}, \textit{Java} and \textit{Go} files and only for models that can be used by official SDK provided by  \textit{OpenAI}, \textit{Gemini}, \textit{Anthropic}, \textit{Minstral}, \textit{xAI}, or \textit{Meta}. 

Just like for Library-Search, Model-Search is implemented as code search based on word matching. To avoid being blocked by models with very short names, (like o1), we search for \textbackslash"word\textbackslash" instead of simply searching for word in a script file. The keyword dictionary, in the case of Model-Search cannot be simply created manually given its higher lenghth. We will need to build it in a separate script, by simply associating each model keyword with the name of the model.

\subsubsection{Data Collection for Library-Search and Model-Search}
The script (mine.py) employs a dynamic chunking strategy based on size to be able to retrieve all occurrences of files satisfying certain queries. This query can be based on model names for Model-Search or on imports for Library-Search. A dedicated part of the script is used to manage rate-limiting from GitHub by pausing the execution each time we hit it until the next reset time. We query the endpoint a page at a time, with each page containing one hundred items, until we finish all results. If we get one thousand or more results, we stop and launch an exception because we could not retrieve all occurrences of the searched pattern. Lastly, based on the number of results, we adjust the size interval so that we can go faster when the queries are mostly empty, while not reaching the thousand cap when the results are dense. 

To complete the data collection step we need to run the same function (collect\_repo\_by\_language) two time for each language: in the first run we search for library imports, while in the second run for model names. Given the objective of retrieving everything and the long list of models to search for, this data collection step was the most time consuming of all, needing almost a week to complete.

\subsubsection{Data Collection for Attribute-Search}
Starting from the list of collected repos we randomly sample five hundred repository and query the GitHub endpoint for attributes of interest. The script is quite simple and it follows more or less the same logic outlined by Keyword-Search. The github\_get function allows to manage authentication errors and rate-limiting while repeating the request in case of unstable internet connection. The execution process did not take much time, and the only error we encountered is the 404 not found error. This error is probably caused by deleted projects or projects that have been made private since the execution of Model and Library-Search. To avoid this error we just repeat the sampling process multiple times until we reach a run without the error, which should not take too many runs since this error is rare (you encounter one such repo in five hundred).

\subsection{Data Analysis}
In this section we discuss how we analyzed the data collected during the execution process, how and which graphs were built and the statistical methods that were applied.

\subsubsection{Data Analysis for Keyword-Search}
With the collected data from the one hundred most starred repository created each month we tried to respond to the question of the popularity of LLM-related projects and their evolution. We analyzed in total five languages: \textit{Python}, \textit{Java}, \textit{C\#}, \textit{JavaScript} and \textit{Go}. We analyzed the name, the description and the topics for each repository and classified a repository as LLM-related if they contained at least a word from our list of keywords. We then just show the proportion of LLM-related projects grouping by the month of creation of the project.

\subsubsection{Data Analysis for Library-Search and Model-Search}
The data collected by the mining script is memorized in the format of a JSON file containing a list of dicts. The dicts contain the name of the repository, URL HTML that is a link to the human readable page of the project, and a label dict. The label dict contains for each label (the keyword that was found) a list of file path. Each file path represents the location where the keyword was found. We are interested in knowing the relationship between model data and the library data, so we need to first combine the two data dicts before any further actions.

The combine function combines the two JSON files to derive data about Model-Library correlation. First, we consider the repositories appearing in both lists: these repositories contain both the library import and the model reference, and as such, we can label them with both. In the case where the import and the model reference are in the same file (we also remember the filepaths), we can pair the two labels up. If we only found the import in a repository, we label it with the name of the provider, while if we only found a reference to a model, we label it with unknown library. In this way, we obtained a list of repositories with informative labels that could then be used in the analysis phase. 

This step is done by running the combine function in the combine.py file. A special dict called model-library map is required for the script to be able to match models to the correct SDK. We built this Python dictionary starting from the previous list of models by creating a key for each model and associating it to an empty list of strings which is then updated by appending possible parent SDKs. In this way only correct SDK-Model matchings will be created in the final JSON file.

%------------------------------------------------

\section{Results}

[\textbf{AGGIUNGERE LA PARTE DEI RISULTATI CON TUTTE LE CONSIDERAZIONI, IMMAGINI E GRAFICI DEL CASO, QUESTA E LA PARTE DI DATA ANALYSIS E' DOVE ANDREMO A FARE IL PADDING PIU' SPUDORATO DELLA STORIA}]

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_prop\_python.png}
    \caption{top models prop python}
    \label{fig:python_tmp}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_python.png}
    \caption{top models python}
    \label{fig:python_tm}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{llm\_model\_usage\_python.png}
    \caption{model usage python}
    \label{fig:python_lmu}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_prop\_java.png}
    \caption{top models prop java}
    \label{fig:java_tmp}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_java.png}
    \caption{top models java}
    \label{fig:java_tm}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{llm\_model\_usage\_java.png}
    \caption{model usage java}
    \label{fig:java_lmu}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_prop\_go.png}
    \caption{top models prop go}
    \label{fig:go_tmp}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{top\_models\_go.png}
    \caption{top models go}
    \label{fig:go_tm}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{llm\_model\_usage\_go.png}
    \caption{model usage go}
    \label{fig:go_lmu}
\end{figure*}

%------------------------------------------------

\section{Discussion}

This statement requires citation. This statement requires multiple citations . This statement contains an in-text citation, for directly referring to a citation like so:.

\subsection{Subsection One}

Suspendisse potenti. Vivamus suscipit dapibus metus. Proin auctor iaculis ex, id fermentum lectus dapibus tristique. Nullam maximus eros eget leo pretium dapibus. Nunc in auctor erat, id interdum risus. Suspendisse aliquet vehicula accumsan. In vestibulum efficitur dictum. Sed ultrices, libero nec fringilla feugiat, elit massa auctor ligula, vehicula tempor ligula felis in lectus. Suspendisse sem dui, pharetra ut sodales eu, suscipit sit amet felis. Donec pretium viverra ante, ac pulvinar eros. Suspendisse gravida consectetur urna. Pellentesque vitae leo porta, imperdiet eros eget, posuere sem. Praesent eget leo efficitur odio bibendum condimentum sit amet vel ex. Nunc maximus quam orci, quis pulvinar nibh eleifend ac. Quisque consequat lacus magna, eu posuere tellus iaculis ac. Sed vitae tortor tincidunt ante sagittis iaculis.

\subsection{Subsection Two}

Nullam mollis tellus lorem, sed congue ipsum euismod a. Donec pulvinar neque sed ligula ornare sodales. Nulla sagittis vel lectus nec laoreet. Nulla volutpat malesuada turpis at ultricies. Ut luctus velit odio, sagittis volutpat erat aliquet vel. Donec ac neque eget neque volutpat mollis. Vestibulum viverra ligula et sapien bibendum, vel vulputate ex euismod. Curabitur nec velit velit. Aliquam vulputate lorem elit, id tempus nisl finibus sit amet. Curabitur ex turpis, consequat at lectus id, imperdiet molestie augue. Curabitur eu eros molestie purus commodo hendrerit. Quisque auctor ipsum nec mauris malesuada, non fringilla nibh viverra. Quisque gravida, metus quis semper pulvinar, dolor nisl suscipit leo, vestibulum volutpat ante justo ultrices diam. Sed id facilisis turpis, et aliquet eros.

\subsubsection{Subsubsection Example}

Duis venenatis eget lectus a aliquet. Integer vulputate ante suscipit felis feugiat rutrum. Aliquam eget dolor eu augue elementum ornare. Nulla fringilla interdum volutpat. Sed tincidunt, neque quis imperdiet hendrerit, turpis sapien ornare justo, ac blandit felis sem quis diam. Proin luctus urna sit amet felis tincidunt, sed congue nunc pellentesque. Ut faucibus a magna faucibus finibus. Etiam id mi euismod, auctor nisi eget, pretium metus. Proin tincidunt interdum mi non interdum. Donec semper luctus dolor at elementum. Aenean eu congue tortor, sed hendrerit magna. Quisque a dolor ante. Mauris semper id urna id gravida. Vestibulum mi tortor, finibus eu felis in, vehicula aliquam mi.

Aliquam arcu turpis, ultrices sed luctus ac, vehicula id metus. Morbi eu feugiat velit, et tempus augue. Proin ac mattis tortor. Donec tincidunt, ante rhoncus luctus semper, arcu lorem lobortis justo, nec convallis ante quam quis lectus. Aenean tincidunt sodales massa, et hendrerit tellus mattis ac. Sed non pretium nibh. 

Donec cursus maximus luctus. Vivamus lobortis eros et massa porta porttitor. Nam vitae suscipit mi. Pellentesque ex tellus, iaculis vel libero at, cursus pretium sapien. Curabitur accumsan velit sit amet nulla lobortis, ut pretium ex aliquam. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.

%----------------------------------------------------------------------------------------
%	 REFERENCES
%----------------------------------------------------------------------------------------

\printbibliography % Output the bibliography

%----------------------------------------------------------------------------------------

\end{document}
